{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOL4Wt3FMH9mzMn7Q9Feouz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7ed1d3e02e5641cfbcda1ba39c4dd05e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e60b860617fc4151a8be0273532e157d","IPY_MODEL_1620dae2bd344adfb17f6fa2104f3516","IPY_MODEL_7a76449916244ff1a112c257326cf9ac"],"layout":"IPY_MODEL_7d73c5ce27384688b7a028a641d16620"}},"e60b860617fc4151a8be0273532e157d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5962b9917394db0a24abdb59b80b0d7","placeholder":"​","style":"IPY_MODEL_802adde592494d86a59b4c0a23c3c35c","value":"100%"}},"1620dae2bd344adfb17f6fa2104f3516":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12ab468a16244a7684c7cc4ef2ab014b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6ff3600e81f4ff4b126b46ace65547e","value":1}},"7a76449916244ff1a112c257326cf9ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4fbfd8e636d4e42bc18f01d990fdf11","placeholder":"​","style":"IPY_MODEL_1cf1912b982b4329a91b14fc441d36f6","value":" 1/1 [00:00&lt;00:00, 25.82it/s]"}},"7d73c5ce27384688b7a028a641d16620":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5962b9917394db0a24abdb59b80b0d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"802adde592494d86a59b4c0a23c3c35c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12ab468a16244a7684c7cc4ef2ab014b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6ff3600e81f4ff4b126b46ace65547e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4fbfd8e636d4e42bc18f01d990fdf11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cf1912b982b4329a91b14fc441d36f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cLIaMWJzGZhD","executionInfo":{"status":"ok","timestamp":1669027991754,"user_tz":-210,"elapsed":3918,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"5e2ab8b2-b1cf-4366-b5c6-5ee617d3c96b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["cd /content/gdrive/MyDrive/English_LDA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgZSyLjtGaES","executionInfo":{"status":"ok","timestamp":1669027991755,"user_tz":-210,"elapsed":9,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"94587596-ac94-4c9c-8453-17a949d69a0b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/English_LDA\n"]}]},{"cell_type":"code","source":["!pip install datasets \n","!conda install -c huggingface -c conda-forge datasets\n","\n","\n","!pip install convert_numbers\n","\n","!pip install nltk\n","\n","\n","!pip install  -q parsivar\n","!pip install pyLDAvis==3.2.2\n","\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install transformers[sentencepiece]\n","\n","\n","!pip -q install \"gensim==3.8.1\"\n","!pip install -q python-bidi\n","!pip install -q arabic_reshaper"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nB37XBvjGaGx","executionInfo":{"status":"ok","timestamp":1669012614561,"user_tz":-210,"elapsed":91996,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"aa0e0240-7108-44e3-bd10-0bb22d8fb72c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.7.0-py3-none-any.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 28.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 62.7 MB/s \n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 75.4 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.7.0 huggingface-hub-0.11.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n","/bin/bash: conda: command not found\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hazm\n","  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n","\u001b[K     |████████████████████████████████| 316 kB 19.8 MB/s \n","\u001b[?25hCollecting libwapiti>=0.2.1\n","  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n","\u001b[K     |████████████████████████████████| 233 kB 53.4 MB/s \n","\u001b[?25hCollecting nltk==3.3\n","  Downloading nltk-3.3.0.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 42.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n","Building wheels for collected packages: nltk, libwapiti\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394489 sha256=039a87953eef77c068447164f6017adef116075f98fc17c37fb3089122fb8efe\n","  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154640 sha256=e8412ffb14959c80f06c7637fa56c1ade0c78cc3c13b6c6da98444f74858bb4e\n","  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n","Successfully built nltk libwapiti\n","Installing collected packages: nltk, libwapiti, hazm\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.7\n","    Uninstalling nltk-3.7:\n","      Successfully uninstalled nltk-3.7\n","Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nltk"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting convert_numbers\n","  Downloading convert_numbers-0.4.tar.gz (319 kB)\n","\u001b[K     |████████████████████████████████| 319 kB 23.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: convert-numbers\n","  Building wheel for convert-numbers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for convert-numbers: filename=convert_numbers-0.4-py3-none-any.whl size=3824 sha256=a2b51965897221b2b4fc9e6b4a9529a3078bfa7f906094ae7cc4539fb869f219\n","  Stored in directory: /root/.cache/pip/wheels/c5/fe/9f/1d1fbc91b7c48e9ef7b5327ae787cb83df3152d2d9f01d7e61\n","Successfully built convert-numbers\n","Installing collected packages: convert-numbers\n","Successfully installed convert-numbers-0.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n","Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n","Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n","\u001b[K     |████████████████████████████████| 36.2 MB 366 kB/s \n","\u001b[K     |████████████████████████████████| 1.5 MB 32.9 MB/s \n","\u001b[?25h  Building wheel for parsivar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","hazm 0.7.0 requires nltk==3.3, but you have nltk 3.4.5 which is incompatible.\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyLDAvis==3.2.2\n","  Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 15.8 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (0.38.3)\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.21.6)\n","Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.7.3)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.2.0)\n","Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (2.11.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (2.8.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (0.16.0)\n","Collecting funcy\n","  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.3.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==3.2.2) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==3.2.2) (1.15.0)\n","Building wheels for collected packages: pyLDAvis\n","  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135618 sha256=9a7812da41df332a96234089a289a7480bc7fba0250fff50afec8185c3ad5796\n","  Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284\n","Successfully built pyLDAvis\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-1.17 pyLDAvis-3.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 24.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.13.2 transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 26.5 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.24.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.13.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.11.0)\n","Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.19.6)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.97)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers[sentencepiece]) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2022.9.24)\n","\u001b[K     |████████████████████████████████| 24.2 MB 1.5 MB/s \n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["##Download tweet Dataset from Huggingface\n","https://huggingface.co/datasets/tweet_eval\n","\n","https://huggingface.co/docs/datasets/loading"],"metadata":{"id":"k6WjUP8bQxRn"}},{"cell_type":"code","source":["from datasets import list_datasets, load_dataset\n","from pprint import pprint\n","from datasets import Dataset\n","import pandas as pd \n","\n","# datasets_list = list_datasets() \n","# pprint(datasets_list,compact=True) \n","\n","dataset = load_dataset('ethos','binary')  \n","ethos_train = load_dataset('ethos','binary',split='train')\n","# ethos_train['text']\n","# ethos_validation = load_dataset('ethos','binary',split='validation')\n","\n","df = pd.DataFrame({\"text\": ethos_train['text']})\n","df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451,"referenced_widgets":["7ed1d3e02e5641cfbcda1ba39c4dd05e","e60b860617fc4151a8be0273532e157d","1620dae2bd344adfb17f6fa2104f3516","7a76449916244ff1a112c257326cf9ac","7d73c5ce27384688b7a028a641d16620","e5962b9917394db0a24abdb59b80b0d7","802adde592494d86a59b4c0a23c3c35c","12ab468a16244a7684c7cc4ef2ab014b","f6ff3600e81f4ff4b126b46ace65547e","d4fbfd8e636d4e42bc18f01d990fdf11","1cf1912b982b4329a91b14fc441d36f6"]},"id":"u8aURc0LGaLg","executionInfo":{"status":"ok","timestamp":1668967849929,"user_tz":-210,"elapsed":697,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"62bf8111-9502-4b5e-ddfa-18dc23237468"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset ethos (/root/.cache/huggingface/datasets/ethos/binary/1.0.0/898d3d005459ee3ff80dbeec2f169c6b7ea13de31a08458193e27dec3dd9ae38)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed1d3e02e5641cfbcda1ba39c4dd05e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset ethos (/root/.cache/huggingface/datasets/ethos/binary/1.0.0/898d3d005459ee3ff80dbeec2f169c6b7ea13de31a08458193e27dec3dd9ae38)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                text\n","0          You should know women's sports are a joke\n","1    You look like Sloth with deeper Down’s syndrome\n","2  You look like Russian and speak like Indian. B...\n","3               Women deserve to be abused, I guess.\n","4  Women are made for making babies and cooking d...\n","5      why haven't you faggots taken the bi-pill yet\n","6  Why don’t you go beck your god to murder you? ...\n","7  why does that african american monkey hold gun...\n","8  Why does Bulgaria exist as a country? It's one...\n","9  Why do negroes tear down their own neighborhoo..."],"text/html":["\n","  <div id=\"df-86c563e0-4631-41b8-823e-1f0bab63018b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>You should know women's sports are a joke</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>You look like Sloth with deeper Down’s syndrome</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>You look like Russian and speak like Indian. B...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Women deserve to be abused, I guess.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Women are made for making babies and cooking d...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>why haven't you faggots taken the bi-pill yet</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Why don’t you go beck your god to murder you? ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>why does that african american monkey hold gun...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Why does Bulgaria exist as a country? It's one...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Why do negroes tear down their own neighborhoo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86c563e0-4631-41b8-823e-1f0bab63018b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-86c563e0-4631-41b8-823e-1f0bab63018b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-86c563e0-4631-41b8-823e-1f0bab63018b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["df['text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4KjdcLoPEOY","executionInfo":{"status":"ok","timestamp":1668968167523,"user_tz":-210,"elapsed":448,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"fa309c64-d6eb-48cc-abbe-ff16e12b9a47"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0              You should know women's sports are a joke\n","1        You look like Sloth with deeper Down’s syndrome\n","2      You look like Russian and speak like Indian. B...\n","3                   Women deserve to be abused, I guess.\n","4      Women are made for making babies and cooking d...\n","                             ...                        \n","993     From the midnight sun where the hot springs blow\n","994                          Don't say I'm not your type\n","995     And therefore never send to know for whom the...\n","996                        And I can't stand another day\n","997     All values, unless otherwise stated, are in U...\n","Name: text, Length: 998, dtype: object"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["df.to_csv('tweet.csv')  "],"metadata":{"id":"RVN5rusLPEQv","executionInfo":{"status":"ok","timestamp":1668968253941,"user_tz":-210,"elapsed":569,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["##English LDA"],"metadata":{"id":"JgUrRj0-QrsX"}},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","\n","import re\n","from typing import List, Optional, Union, Callable\n","import string\n","\n","# from bertopic import BERTopic\n","\n","import numpy as np\n","from numpy import array\n","\n","from wordcloud import WordCloud\n","\n","import json\n","###############################################################\n","from typing import List\n","import convert_numbers\n","\n","from gensim.corpora.dictionary import Dictionary\n","from nltk.util import ngrams\n","from gensim.models.ldamodel import LdaModel\n","from gensim.models.coherencemodel import CoherenceModel\n","import matplotlib.pyplot as plt\n","from gensim.models.ldamodel import LdaModel\n","from gensim.models.coherencemodel import CoherenceModel\n","\n","from wordcloud import WordCloud\n","from PIL import Image, ImageFont, ImageDraw\n","from bidi.algorithm import get_display\n","import arabic_reshaper\n","import pyLDAvis.gensim\n","\n","import convert_numbers\n","\n","import time"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5O6LfoCRQrHP","executionInfo":{"status":"ok","timestamp":1669027996661,"user_tz":-210,"elapsed":2790,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"08bacb13-3395-4060-8552-577355b3185d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["text = \"I'm zahra nafarieh\"\n","my_stopwords=[]\n","file = open('./my_stop_words_english.txt').read()\n","for x in file.split('\\n'):\n","  my_stopwords.append(x) \n","stop_words = set(my_stopwords)\n","\n","\n","word_tokens = word_tokenize(text)\n","filtered_sentence = [w for w in word_tokens if  (w.lower() not in stop_words) and (len(w)>1)]\n","\n","\n","\n","\n","filtered_sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ctCZPssq-PG","executionInfo":{"status":"ok","timestamp":1669025560932,"user_tz":-210,"elapsed":632,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"e5f94c00-c3b4-4548-f931-226244a51642"},"execution_count":141,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['zahra', 'nafarieh']"]},"metadata":{},"execution_count":141}]},{"cell_type":"code","source":["def remove_stopwords(text):\n","\n","  # stop_words = set(stopwords.words('english'))\n","\n","\n","\n","  my_stopwords=[]\n","  file = open('./my_stop_words_english.txt').read()\n","  for x in file.split('\\n'):\n","    my_stopwords.append(x) \n","  stop_words = set(my_stopwords)\n","\n","\n","  word_tokens = word_tokenize(text)\n","  filtered_sentence = [w for w in word_tokens if  (w.lower() not in stop_words) and (len(w)>1)]\n","  \n","  filtered_text = ' '.join(filtered_sentence)\n","  return filtered_text\n","\n","def remove_emoji(text): \n","    emoji_pattern = re.compile(\"[\"\n","                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                u\"\\U00002702-\\U000027B0\"\n","                u\"\\U000024C2-\\U0001F251\"\n","                u\"\\U0001f926-\\U0001f937\"\n","                u'\\U00010000-\\U0010ffff'\n","                u\"\\u200d\"\n","                u\"\\u200c\"\n","                u'\\u200f'\n","                u'\\u200e'\n","                u\"\\u2640-\\u2642\"\n","                u\"\\u2600-\\u2B55\"\n","                u\"\\u23cf\"\n","                u\"\\u23e9\"\n","                u\"\\u231a\"\n","                u\"\\u3030\"\n","                u\"\\ufe0f\"\n","                u\"\\u202b\"\n","\n","                u\"\\U000E006E|\" \\\n","                u\"\\U000E007F|\" \\\n","                u\"\\U000E0073|\" \\\n","                u\"\\U000E0063|\" \\\n","                u\"\\U000E0074|\" \\\n","                u\"\\U000E0077|\" \\\n","                u\"\\U000E006C\"\n","\n","    \"]+\", flags=re.UNICODE)\n","    \n","    return emoji_pattern.sub(r' ', text)\n","\n","def remove_number(text):\n","    \"\"\" Remove number in the input text \"\"\"\n","    processed_text = re.sub('\\d+', '', text)\n","    return processed_text\n","\n","def remove_url(text):\n","    \"\"\" Remove url in the input text \"\"\"\n","    return re.sub('(www|http)\\S+', '', text)\n","\n","def remove_punctuation(input_text: str, punctuations: Optional[str] = None) -> str:\n","    \"\"\"\n","    Removes all punctuations from a string, as defined by string.punctuation or a custom list.\n","    For reference, Python's string.punctuation is equivalent to '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~'\n","    \"\"\"\n","    if punctuations is None:\n","        punctuations = string.punctuation\n","    processed_text = input_text.translate(str.maketrans('', '', punctuations))\n","    return processed_text\n","\n","\n","def remove_link(text): \n","    return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', str(text))\n","\n","def remove_tabs(text): \n","    return re.sub(r'[\\n\\r\\t]', '', str(text))\n","\n","def remove_email(text): \n","    return re.sub(r'\\S+@\\S+', '', str(text))\n","\n","# def remove_englishword(text): \n","#     return re.sub(r'[A-Za-z0-9]+', '', str(text))\n","\n","def remove_chars(text): \n","    # return re.sub(r'\\.(?!\\d)', '', str(text))\n","    # return  re.sub(r'[$+&+;+]|[><!+،:,\\(\\).+،+٬+,+٬]|[-+]|[…]|[\\[\\]»«//]|[\\\\]|[#+]|[_+]|[٪+]|[%]|[*+]|[؟+]|[?+]|[\"\"]|@|' '', '', str(text))\n","    return  re.sub(r'[$+&+;+]|[><!+،:,\\(\\).+،+٬+,+]|[-+]|[…]|[\\[\\]»«//]|[\\\\]|[#+]|[_+]|[٪+]|[%]|[*+]|[؟+]|[?+]|[\"\"]|@', ' ', str(text))\n","\n","\n","def remove_extraspaces(text):\n","    return re.sub(r' +', ' ', text)\n","\n","def remove_extranewlines(text):\n","    return re.sub(r'\\n\\n+', '\\n\\n', text)\n","\n","def handle_clear_more_triple_chars(text):\n","    # remove any char that appear more than 2 times Continuously except whitespaces, tabs and newlines\n","    doc_string=re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", text)\n","    return doc_string\n","\n","\n","\n","def remove_nested_keywords(topics):\n","\n","    keys = list(topics.keys())\n","    values = list(topics.values())\n","    for i, sub_item in enumerate(keys):\n","        for j, item in enumerate(keys):\n","            if i==j:\n","                continue\n","            elif sub_item in item:\n","                keys[i] = \"-1-1+1\"\n","                values[i] = \"-1-1+1\"\n","\n","    keys = list(filter(lambda a: a != \"-1-1+1\", keys))\n","    values = list(filter(lambda a: a != \"-1-1+1\", values))\n","\n","    new = {}\n","    for i in range(len(keys)):\n","        try:\n","            new_key = keys[i]\n","            if len(new_key.split())>1 and new_key.split().count(\"_\") > 1 and not new_key.split()[-1]!=\"_\" and not new_key.split()[0] != \"_\":\n","                new_key = '\\u200c'.join(new_key.split())\n","            if len(new_key) > 3:\n","                new[new_key] = values[i]\n","        except: continue\n","    return new\n","\n","\n","\n","def preprocess(text):\n","    text = remove_emoji(text)\n","    text = remove_stopwords(text)\n","    text = remove_link(text)\n","    text = remove_tabs(text) \n","    text = remove_email(text) \n","    # text = remove_englishword(text) \n","    text = remove_chars(text)\n","    text = remove_extraspaces(text) \n","    text = remove_extranewlines(text) \n","    text = remove_number(text)\n","    text = remove_url(text)\n","    text = remove_punctuation(text)\n","\n","\n","    text = handle_clear_more_triple_chars(text)\n","    return text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPoMA4zjPESz","executionInfo":{"status":"ok","timestamp":1669027998905,"user_tz":-210,"elapsed":453,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"a15b95cf-332e-4426-9e56-05c9eba68d62"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:57: DeprecationWarning: invalid escape sequence \\d\n","<>:62: DeprecationWarning: invalid escape sequence \\S\n","<>:57: DeprecationWarning: invalid escape sequence \\d\n","<>:62: DeprecationWarning: invalid escape sequence \\S\n","<>:57: DeprecationWarning: invalid escape sequence \\d\n","<>:62: DeprecationWarning: invalid escape sequence \\S\n","<ipython-input-4-223194c816fe>:57: DeprecationWarning: invalid escape sequence \\d\n","  processed_text = re.sub('\\d+', '', text)\n","<ipython-input-4-223194c816fe>:62: DeprecationWarning: invalid escape sequence \\S\n","  return re.sub('(www|http)\\S+', '', text)\n"]}]},{"cell_type":"code","source":["def wordcloud(topics,mask_path,font_path):\n","        image_address = \"/content/gdrive/MyDrive/English_LDA/English_LDA.png\"\n","        mask = np.array(Image.open(mask_path))\n","        wordcloud = WordCloud(max_font_size=80, background_color=\"white\", font_path=font_path,\n","                              max_words=80, mask=mask, \n","                              margin=10, height=800, width=800, colormap=\"Dark2\", prefer_horizontal=1)\n","        # new_topics = {}\n","        # # box_size = draw.textsize(word, font=transposed_font)\n","        # for x, y in topics.items():\n","        #     try:\n","        #         new_topics[get_display(arabic_reshaper.reshape(x))] = y\n","        #     except: continue\n","        # print(new_topics)\n","        # topics = new_topics\n","        wordcloud.generate_from_frequencies(topics)\n","        wordcloud.to_file(image_address)\n","        image = Image.open(image_address)\n","        image.thumbnail((800, 800), Image.ANTIALIAS)\n","        image = image.save(image_address, 'png', quality=100)\n","        # plt.imshow(wordcloud)\n","        plt.show()\n","        \n","        return topics"],"metadata":{"id":"ff6Oe_5YQqZw","executionInfo":{"status":"ok","timestamp":1669028001031,"user_tz":-210,"elapsed":3,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def create_dict(keys, values):\n","    topics = {}\n","    for key in set(keys):\n","        topics[key] = ([values[i] for i, value in enumerate(keys) if value == key])\n","    \n","    new_topics = {}\n","    for key in topics.keys():\n","        val = sum(topics[key])\n","        if val and len(key)>1:\n","            new_topics[key] = val  * 10000 \n","        else: continue\n","    return new_topics\n","\n","def remove_unusful_topics(topics):\n","    return {x: y for x, y in topics.items() if not x.endswith(\"های\") and not x.endswith(\"هایی\")}\n","\n","def remove_nested_keywords(topics):\n","    keys = list(topics.keys())\n","    values = list(topics.values())\n","    for i, sub_item in enumerate(keys):\n","        for j, item in enumerate(keys):\n","            if i==j:\n","                continue\n","            elif sub_item in item:\n","                keys[i] = \"-1-1+1\"\n","                values[i] = \"-1-1+1\"\n","\n","    keys = list(filter(lambda a: a != \"-1-1+1\", keys))\n","    values = list(filter(lambda a: a != \"-1-1+1\", values))\n","\n","    new = {}\n","    for i in range(len(keys)):\n","        try:\n","            new_key = keys[i]\n","            if len(new_key.split())>1 and new_key.split().count(\"_\") > 1 and not new_key.split()[-1]!=\"_\" and not new_key.split()[0] != \"_\":\n","                new_key = '\\u200c'.join(new_key.split())\n","            if len(new_key) > 3:\n","                new[new_key] = values[i]\n","        except: continue\n","    return new"],"metadata":{"id":"as-SNLufi9hB","executionInfo":{"status":"ok","timestamp":1669028004031,"user_tz":-210,"elapsed":450,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def ngram_convertor(sentence, stopwords, n=3, flag=1):\n","    ngram_list= []\n","    # tok= []\n","    if len(sentence)>1:\n","      if len(sentence.split())>1 : #sentence has more than one word\n","        if n== 2:\n","          sentence = remove_stopwords(sentence)\n","        ngram_sentence = ngrams(sentence.split(), n)\n","        for tuple_ngram in ngram_sentence:\n","          ngram_list.append(\" \".join(tuple_ngram))\n","        if flag == 1 :\n","          #Remove stopword in sentence\n","          sentence_pre = remove_stopwords(sentence)\n","          for item_tokens in word_tokenize(sentence_pre):\n","            if len(item_tokens)>1 and (item_tokens not in stopwords):\n","              ngram_list.append(item_tokens) #add tokens\n","        # ngram_list.append(tok)\n","    return ngram_list"],"metadata":{"id":"wfQXwNvB000E","executionInfo":{"status":"ok","timestamp":1669028006526,"user_tz":-210,"elapsed":417,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def tokenize_ngram(stopwords,df,docs):\n","  my_docs =array(df['text'].dropna())\n","\n","  final_ngram_list= []\n","  for idx in range(len(my_docs)):\n","    flag= 1 #just one time tokenize\n","    for n in [2,3]: #treegram\n","      sentence= my_docs[idx]\n","      # if len(sentence)>1 : #sentence not null\n","      # print(sentence) \n","      final_ngram_list.append(ngram_convertor(sentence,stopwords,n,flag))\n","      # print(final_ngram_list)\n","      flag= 0\n","    if len(final_ngram_list)>0 :\n","      docs[idx]= final_ngram_list\n","      final_ngram_list= []\n","\n","  return docs"],"metadata":{"id":"Obss3AsWi1vS","executionInfo":{"status":"ok","timestamp":1669028007056,"user_tz":-210,"elapsed":3,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def final_corpus(docs):\n","  zz = docs\n","  zx= []\n","  for i in range(len(zz)):\n","    try:\n","      if len(zz[i][0])>0 or len(zz[i][1])>0 :\n","        zx.append(zz[i][0]+zz[i][1])\n","    except:\n","      print(\"*******************\")\n","\n","  xx=[]\n","  for i in zx :\n","    if type(i) == list:\n","      xx.append(i)\n","    else:\n","      xx.append([i])\n","\n","  #https://radimrehurek.com/gensim/corpora/dictionary.html\n","  dictionary = Dictionary(xx)\n","  print(\"***************************\")\n","  dictionary.filter_extremes(no_below=1, no_above=1)\n","  #Create dictionary and corpus required for Topic Modeling\n","  corpus = [dictionary.doc2bow(zzz) for zzz in xx]\n","\n","  return corpus,dictionary"],"metadata":{"id":"5F_HThdqilNK","executionInfo":{"status":"ok","timestamp":1669028009796,"user_tz":-210,"elapsed":384,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def final_lda(corpus,dictionary,num_topic):\n","  # Set parameters.\n","  num_topics = num_topic\n","  chunksize = 500 #Number of Document to memory\n","  passes = 5 #how many time document can see \n","  eval_every = 1  ####################\n","  iterations= 100\n","\n","  temp = dictionary[0]  # only to \"load\" the dictionary.\n","  id2word = dictionary.id2token\n","\n","  lda_model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n","                        alpha='auto', eta='auto', \\\n","                        iterations=iterations, num_topics=num_topics, \\\n","                        passes=passes, eval_every=eval_every)\n","\n","  keys = []\n","  values= []\n","\n","  for i, topic in lda_model.show_topics(formatted=True, num_topics=10,num_words=10):\n","    print(i, topic)\n","    topic = topic.split(\"+\")\n","    for item in topic:\n","        item = item.split(\"\\\"\")\n","        val = item[0].replace(\"*\", \"\").replace(\" \", \"\")\n","        try:\n","            value = float(val)\n","            values.append(value)\n","            keys.append(item[1])\n","        except: continue   \n","\n","  return topic,keys,values"],"metadata":{"id":"oUEQiM49hJ6D","executionInfo":{"status":"ok","timestamp":1669028015561,"user_tz":-210,"elapsed":400,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["##Stopword List"],"metadata":{"id":"qhv1U7mz_oSJ"}},{"cell_type":"code","source":["# from nltk.corpus import stopwords\n","# stopwords = set(stopwords.words('english'))\n","\n","if \"dont\" in my_stopwords :\n","  print(\"YYYYYY\")"],"metadata":{"id":"A2DS8_3c_u7m","executionInfo":{"status":"ok","timestamp":1669025013244,"user_tz":-210,"elapsed":372,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":126,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","# df,stopwords= read_files(filename)\n","# df = pd.read_csv(str(filename))\n","my_stopwords=[]\n","my_stopwords = list(stopwords.words('english'))\n","\n","print(len(my_stopwords))\n","\n","\n","file = open('./stop_words_english.txt').read()\n","for x in file.split('\\n'):\n","  my_stopwords.append(x) \n","\n","len(list(my_stopwords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nfB4RBeGxg3","executionInfo":{"status":"ok","timestamp":1669025261908,"user_tz":-210,"elapsed":479,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"4b43fc94-631a-476d-f034-c479189fdea6"},"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["179\n"]},{"output_type":"execute_result","data":{"text/plain":["1030"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["aa= [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",\"do'nt\",\"dont\",\"its\",\"it's\"]\n","for i in aa:\n","  # print(i)\n","  my_stopwords.append(i)"],"metadata":{"id":"9tnDVuHpnzpG","executionInfo":{"status":"ok","timestamp":1669025265335,"user_tz":-210,"elapsed":585,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":["# open file in write mode\n","with open(r'my_stop_words_english.txt', 'w') as fp:\n","    for item in my_stopwords:\n","        # write each item on a new line\n","        fp.write(\"%s\\n\" % item)\n","    print('Done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6BKn1TcRUa5P","executionInfo":{"status":"ok","timestamp":1669025275034,"user_tz":-210,"elapsed":466,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"c0638ccf-947e-484a-bd19-96961447d59e"},"execution_count":135,"outputs":[{"output_type":"stream","name":"stdout","text":["Done\n"]}]},{"cell_type":"code","source":["my_stopwords=[]\n","file = open('./my_stop_words_english.txt').read()\n","for x in file.split('\\n'):\n","  my_stopwords.append(x) \n","\n","len(my_stopwords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QF9XWFkKUa77","executionInfo":{"status":"ok","timestamp":1669025285531,"user_tz":-210,"elapsed":428,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"9dfeda39-2559-42f1-b9b9-0b07ab15fb3e"},"execution_count":136,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2195"]},"metadata":{},"execution_count":136}]},{"cell_type":"code","source":["# '√∞¬ü¬á¬™√∞¬ü¬á¬∫√∞¬ü¬á¬™√∞¬ü¬á¬∫√∞¬ü¬á¬™√∞¬ü¬á¬∫√∞¬ü¬á¬™√∞¬ü¬á¬∫√∞¬ü¬á¬™√∞¬ü¬á¬∫√∞¬ü¬á¬™√∞¬ü¬á¬∫√∞¬ü¬á¬™√∞¬ü¬á¬∫√∞¬ü¬á¬™√∞¬ü¬á¬∫√¢¬ö¬î√Ø¬∏¬è√¢¬ö¬î√Ø¬∏¬è√¢¬ö¬î√Ø¬∏¬è√¢¬ö¬î√Ø¬∏¬è√¢¬ö¬î√Ø¬∏¬è√¢¬ö¬î√Ø¬∏¬è√∞¬ü¬ó¬°√∞¬ü¬ó¬°√∞¬ü¬ó¬°√∞¬ü¬ó¬°'"],"metadata":{"id":"Wlz5Mh_rccjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Run function"],"metadata":{"id":"xzwNobav_r9f"}},{"cell_type":"code","source":["def run(filename,mask_path,font_path,num_topic):\n","\n","  from nltk.corpus import stopwords\n","\n","  df = pd.read_csv(str(filename))\n","\n","  my_stopwords=[]\n","  file = open('./my_stop_words_english.txt').read()\n","  for x in file.split('\\n'):\n","    my_stopwords.append(x) \n","  my_stopwords = set(my_stopwords)\n","\n","  print(\"preprocessing started\")\n","  \n","  start_pre = time.time()\n","  print(\"1\")\n","  \n","  df['text'] = df['text'].apply(preprocess)\n","  df['text'] = df['text'].dropna()\n","  print(\"2\")\n","  # df['comment'] = df['comment'].dropna()\n","  df['text']= pd.DataFrame(df[df['text'].map(len) > 2][\"text\"])\n","  final_df = pd.DataFrame(df['text'].dropna()).reset_index()\n","  print(\"3\")\n","  docs =array(df['text'])\n","  print(\"4\")\n","  end_pre = time.time() - start_pre\n","  print(\"preprocessing finished: \" + str(end_pre) + \"\\n\")\n","  ########\n","  start_lda = time.time()\n","  print(\"lda started\")\n","  docs= tokenize_ngram(my_stopwords,final_df,docs)\n","  # print(docs)\n","  corpus,dictionary= final_corpus(docs)\n","  print(5)\n","  topic,keys,values= final_lda(corpus,dictionary,num_topic)\n","  end_lda = time.time() - start_lda\n","  print(\"lda finished: \" + str(end_lda) + \"\\n\")\n","  ########\n","  topics= create_dict(keys, values)\n","  topics= remove_unusful_topics(topics)\n","  # topics= remove_nested_keywords(topics)\n","  wordcloud(topics,mask_path,font_path)\n","  return topics\n","\n","result = run(\"./tweet.csv\",\"/content/gdrive/MyDrive/TopicModeling/mask-instagram.png\",\"/content/gdrive/MyDrive/TopicModeling/Vazir-Bold.ttf\",num_topic=50)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQI9pZWvQqiQ","executionInfo":{"status":"ok","timestamp":1669028035310,"user_tz":-210,"elapsed":14255,"user":{"displayName":"zahra nafarieh","userId":"04721698450462816555"}},"outputId":"46378542-3f8b-47df-89c3-281acc2b0b22"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["preprocessing started\n","1\n","2\n","3\n","4\n","preprocessing finished: 1.8296728134155273\n","\n","lda started\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"]},{"output_type":"stream","name":"stdout","text":["***************************\n","5\n","29 0.009*\"fucking kill\" + 0.009*\"takes\" + 0.009*\"super\" + 0.009*\"kill\" + 0.008*\"fucking\" + 0.005*\"Stupid\" + 0.005*\"sake\" + 0.004*\"black\" + 0.002*\"sake Just fucking\" + 0.002*\"feminists FUCK sake\"\n","27 0.013*\"fact\" + 0.009*\"brain\" + 0.009*\"gay\" + 0.008*\"realize\" + 0.007*\"people\" + 0.006*\"legs\" + 0.006*\"ass\" + 0.005*\"running\" + 0.005*\"die\" + 0.005*\"gay people\"\n","47 0.013*\"shit\" + 0.011*\"speak\" + 0.005*\"Kill\" + 0.005*\"Holy\" + 0.005*\"Holy shit\" + 0.005*\"Fuck\" + 0.005*\"ill\" + 0.005*\"bitch\" + 0.005*\"scary\" + 0.005*\"LGBTQ\"\n","21 0.014*\"mind\" + 0.012*\"hell\" + 0.008*\"president\" + 0.008*\"illnesses\" + 0.005*\"mental\" + 0.005*\"bitches\" + 0.005*\"puertorican\" + 0.005*\"mental illnesses\" + 0.005*\"awful\" + 0.004*\"american\"\n","46 0.009*\"beat\" + 0.008*\"Fuck\" + 0.008*\"islam\" + 0.007*\"truck\" + 0.007*\"news\" + 0.005*\"genetic\" + 0.005*\"white people nt\" + 0.005*\"didnt\" + 0.005*\"wave\" + 0.004*\"people\"\n","34 0.047*\"people\" + 0.029*\"feel\" + 0.018*\"beautiful\" + 0.013*\"wrong\" + 0.010*\"real\" + 0.009*\"dont\" + 0.009*\"shot\" + 0.008*\"problems\" + 0.007*\"straight\" + 0.007*\"bisexual\"\n","28 0.027*\"video\" + 0.025*\"guys\" + 0.021*\"man\" + 0.011*\"community\" + 0.008*\"music\" + 0.006*\"pain\" + 0.006*\"hear\" + 0.006*\"earth\" + 0.006*\"lady\" + 0.005*\"Indians\"\n","16 0.024*\"life\" + 0.018*\"war\" + 0.017*\"autistic\" + 0.012*\"milk\" + 0.012*\"type\" + 0.011*\"love\" + 0.010*\"send\" + 0.010*\"videos\" + 0.010*\"read\" + 0.009*\"people\"\n","10 0.035*\"love\" + 0.020*\"people\" + 0.012*\"experience\" + 0.010*\"crazy\" + 0.010*\"mother\" + 0.009*\"humans\" + 0.009*\"autism\" + 0.008*\"modern\" + 0.008*\"literally\" + 0.006*\"feel\"\n","45 0.018*\"people\" + 0.015*\"country\" + 0.015*\"time\" + 0.015*\"rights\" + 0.009*\"children\" + 0.008*\"walk\" + 0.008*\"room\" + 0.007*\"dress\" + 0.006*\"ugly\" + 0.006*\"shit\"\n","lda finished: 10.594478845596313\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kill': 90.0,\n"," 'videos': 100.0,\n"," 'awful': 50.0,\n"," 'gay': 90.0,\n"," 'speak': 110.0,\n"," 'modern': 80.0,\n"," 'fact': 130.0,\n"," 'white people nt': 50.0,\n"," 'time': 150.0,\n"," 'Kill': 50.0,\n"," 'mother': 100.0,\n"," 'mental illnesses': 50.0,\n"," 'pain': 60.0,\n"," 'type': 120.0,\n"," 'read': 100.0,\n"," 'president': 80.0,\n"," 'children': 90.0,\n"," 'life': 240.0,\n"," 'people': 1050.0,\n"," 'literally': 80.0,\n"," 'guys': 250.0,\n"," 'Stupid': 50.0,\n"," 'send': 100.0,\n"," 'Indians': 50.0,\n"," 'illnesses': 80.0,\n"," 'mind': 140.0,\n"," 'realize': 80.0,\n"," 'hell': 120.0,\n"," 'fucking': 80.0,\n"," 'feel': 350.00000000000006,\n"," 'bitches': 50.0,\n"," 'problems': 80.0,\n"," 'sake Just fucking': 20.0,\n"," 'news': 70.0,\n"," 'dress': 70.0,\n"," 'american': 40.0,\n"," 'country': 150.0,\n"," 'legs': 60.0,\n"," 'fucking kill': 90.0,\n"," 'man': 210.0,\n"," 'ass': 60.0,\n"," 'truck': 70.0,\n"," 'war': 180.0,\n"," 'sake': 50.0,\n"," 'puertorican': 50.0,\n"," 'didnt': 50.0,\n"," 'walk': 80.0,\n"," 'ugly': 60.0,\n"," 'islam': 80.0,\n"," 'crazy': 100.0,\n"," 'die': 50.0,\n"," 'autism': 90.0,\n"," 'gay people': 50.0,\n"," 'community': 110.0,\n"," 'ill': 50.0,\n"," 'black': 40.0,\n"," 'LGBTQ': 50.0,\n"," 'beautiful': 180.0,\n"," 'experience': 120.0,\n"," 'video': 270.0,\n"," 'shit': 190.0,\n"," 'real': 100.0,\n"," 'milk': 120.0,\n"," 'don\\x80\\x99t': 90.0,\n"," 'humans': 90.0,\n"," 'music': 80.0,\n"," 'beat': 90.0,\n"," 'brain': 90.0,\n"," 'Fuck': 130.0,\n"," 'Holy shit': 50.0,\n"," 'hear': 60.0,\n"," 'room': 80.0,\n"," 'wave': 50.0,\n"," 'lady': 60.0,\n"," 'bisexual': 70.0,\n"," 'shot': 90.0,\n"," 'scary': 50.0,\n"," 'autistic': 170.0,\n"," 'earth': 60.0,\n"," 'running': 50.0,\n"," 'wrong': 130.0,\n"," 'love': 460.0,\n"," 'mental': 50.0,\n"," 'super': 90.0,\n"," 'Holy': 50.0,\n"," 'rights': 150.0,\n"," 'straight': 70.0,\n"," 'genetic': 50.0,\n"," 'feminists FUCK sake': 20.0,\n"," 'takes': 90.0,\n"," 'bitch': 50.0}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"N2QL9jFijGkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K7jpoUxLjqp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E5iof88YjqsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i6xfnVlvjGnK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c958LSz7zGSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"u8g4b1V3zGU6"},"execution_count":null,"outputs":[]}]}